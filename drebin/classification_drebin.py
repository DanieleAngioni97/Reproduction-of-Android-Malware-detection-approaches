import pandas as pd
import numpy as np
import time

# SecML imports

from secml.ml.classifiers import CClassifierSVM

from secml.array import CArray
from secml.core import CCreator
from secml.data import CDataset
from secml.ml import CClassifier
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer as CountV
from sklearn.svm import LinearSVC
import os
from matplotlib import pyplot as plt
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import recall_score, accuracy_score, precision_score
from sklearn.utils import shuffle
import CommonModules as CM
import sys
import argparse
"""
This script performs Drebin's classification using SVM LinearSVC classifier.
Inputs are described in parseargs function.
Recall and Accuracy are calculated 10 times. Each experiment is performed with 66% of training set and 33% of test set, and scores are averaged.
Roc curve is plotted using the last trained classifier from the 10 experiments.
The Outputs are the results text file, and roc curve pdf graph that are located in the drebin directory.
"""


def parseargs():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-md",
        "--maldir",
        help="The path to the .data malware files directory",
        type=str,
        required=True
    )
    parser.add_argument(
        "-gd",
        "--goodir",
        help="The path to the .data goodware files directory",
        type=str,
        required=True
    )
    parser.add_argument(
        "-fs",
        "--filescores",
        help="The name of the file where the results will be written",
        type=str,
        required=True
    )
    parser.add_argument(
        "-roc",
        "--roc",
        help="The name of the file where the roc curve will be saved",
        type=str,
        required=True
    )
    parser.add_argument(
        "-sd",
        "--seed",
        help=
        "(Optional) The random seed used for the experiments (useful for 100% identical results replication)",
        type=int,
        required=False,
        default=np.random.randint(0, 2**32 - 1)
    )
    args = parser.parse_args()
    return args


def encode_features(data_path, features_path):
    malware_path = data_path + "/Malware"
    goodware_path = data_path + "/Goodware"

    # the list of absolute paths of the ".data" malware files
    all_mal_samples = CM.ListFiles(malware_path, Extension)

    # the list of absolute paths of the ".data" goodware files
    all_good_samples = CM.ListFiles(goodware_path, Extension)
    all_sample_names = all_mal_samples + all_good_samples  # combine the two lists

    feature_vectorizer = CountV(
        input='filename',
        tokenizer=lambda x: x.split('\n'),
        token_pattern=None,
        binary=True
    )
    mal_labels = np.ones(len(all_mal_samples))  # label malware as 1
    good_labels = np.empty(len(all_good_samples))
    good_labels.fill(0)  # label goodware as 0
    # concatenate the two lists of labels
    y = np.concatenate((mal_labels, good_labels), axis=0)

    # shuffle the list in each experiment
    all_sample_names, y = shuffle(all_sample_names, y)

    # todo: qui ci sarebbe da fare split temporale, sarebbe da modificare comunque solo qui
    # split the lists into 66% for training and 33% for test.
    x_train, x_test, y_train, y_test = train_test_split(
        all_sample_names, y, test_size=0.33
    )

    # learn the vocabulary dictionary from the training set
    x_train = feature_vectorizer.fit_transform(x_train)
    # transform the test set using vocabulary learned
    x_test = feature_vectorizer.transform(x_test)

    tr = CDataset(CArray(x_train), CArray(y_train))
    ts = CDataset(CArray(x_test), CArray(y_test))

    tr.save(features_path + '/train_drebin')
    ts.save(features_path + '/test_drebin')

# def train_drebin_svm(features_path):
#     tr = CCreator().load(features_path + '/train_drebin.gz')
#     ts = CCreator().load(features_path + '/test_drebin.gz')
#
#     clf = CClassifierSVM()
#     clf.fit(tr.X, tr.Y)

def classification(features_path, file_scores, ROC_GraphName):
    tr = CCreator().load(features_path + '/train_drebin.gz')
    ts = CCreator().load(features_path + '/test_drebin.gz')

    clf = CClassifierSVM()
    clf.fit(tr.X, tr.Y)

    scores_prec, scores_rec, scores_F1 = [], [], []  # empty lists to store precision, recall, F1
    y_predict = clf.predict(ts.X)

    from secml.ml.peval.metrics import CMetricPrecision, CMetricRecall, CMetricF1
    precision_score = CMetricPrecision()
    recall_score = CMetricRecall()
    f1_score = CMetricF1()

    score_rec = recall_score.performance_score(ts.Y, y_predict)  # calculate the recall score
    score_prec = precision_score.performance_score(ts.Y, y_predict)  # calculate the precision score
    score_F1 = f1_score.performance_score(ts.Y, y_predict)  # calculate the F1 score

    scores_rec.append(score_rec)
    scores_prec.append(score_prec)
    scores_F1.append(score_F1)

    save_metrics(scores_rec, scores_prec, scores_F1, file_scores, ROC_GraphName)
    save_ROC(clf, ts)


def save_metrics(scores_rec, scores_prec, scores_F1, file_scores, ROC_GraphName):

    # write the results into the results-run.txt file
    outp = open(file_scores, "w")
    outp.write(
        "\nRecall scores of classifiaction with 0.66 training and 0.33 test\n"
    )
    outp.write(str((scores_rec)) + "\n")
    outp.write("Mean : " + str((np.mean(scores_rec))) + "\n")  # store the mean of the recall
    outp.write("Min : " + str((np.min(scores_rec))) + "\n")
    outp.write("Max : " + str((np.max(scores_rec))) + "\n")

    outp.write(
        "\nPrecision scores of classifiaction with 0.66 training and 0.33 test\n"
    )
    outp.write(str((scores_prec)) + "\n")
    outp.write("Mean : " + str((np.mean(scores_prec))) + "\n")  # store the mean of the recall
    outp.write("Min : " + str((np.min(scores_prec))) + "\n")
    outp.write("Max : " + str((np.max(scores_prec))) + "\n")

    outp.write(
        "\nF1 scores of classifiaction with 0.66 training and 0.33 test\n"
    )
    outp.write(str((scores_F1)) + "\n")  # store the mean of the accuracy
    outp.write("Mean : " + str((np.mean(scores_F1))) + "\n")  # store the mean of the recall
    outp.write("Min : " + str((np.min(scores_F1))) + "\n")
    outp.write("Max : " + str((np.max(scores_F1))) + "\n")
    outp.close()

def save_ROC(clf, ts):
    _, y_scores = clf.predict(ts.X, return_decision_function=True)
    # create empty dictionaries for false positive rate, true positive rate
    fpr, tpr = dict(), dict()
    fpr, tpr, _ = roc_curve(ts.Y.tondarray(), y_scores[:,1].T.flatten().tondarray())  # compute the ROC
    plt.figure(figsize=(5, 4))
    plt.title('Receiver Operating Characteristic')
    plt.grid(color='k', linestyle=':')
    plt.plot(fpr, tpr, '-k')  # plot the roc curve
    plt.xlim([0, 0.1])
    plt.ylim([0, 1])
    plt.ylabel('True Positive Rate')
    plt.xlabel('False Positive Rate')
    # save the roc curve in roc.pdf file
    plt.savefig(ROC_GraphName + ".pdf", bbox_inches='tight')
    # plt.show()


Extension = ".data"  #features files that are generated by GetApkData.py script ends with ".data"

if __name__ == '__main__':
    # Args = parseargs()  #retrieve the parameters
    # malware_path = Args.maldir
    # goodware_path = Args.goodir
    # file_scores = Args.filescores
    # ROC_GraphName = Args.roc
    # SEED = Args.seed
    data_path = "preprocessed/800samples"
    features_path = "preprocessed/800samples"

    file_scores = "preprocessed/800samples/file_scores"
    ROC_GraphName = "preprocessed/800samples/file_roc"

    SEED = 0
    np.random.seed(SEED)

    encode_features(data_path, features_path)

    classification(features_path, file_scores, ROC_GraphName)
